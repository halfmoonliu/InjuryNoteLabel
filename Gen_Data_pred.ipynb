{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Data Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dependecies for the synthetic dataset to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to the dataset which has been categorised into the `train data`, `test data` and finally the `developmental set data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/halfmoonliu/InjuryNoteLabel/main/Data/TrainSetGen.csv\"\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/halfmoonliu/InjuryNoteLabel/main/Data/TestSetGen.csv\"\n",
    ")\n",
    "dev_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/halfmoonliu/InjuryNoteLabel/main/Data/DevSetGen.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# train_df['event'].hist()\n",
    "\n",
    "# # Adding titles and labels to the histogram\n",
    "# plt.title('Event Distribution in Synthetic Data')\n",
    "# plt.xlabel('Event')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenising synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained tokenizer\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183856\n",
      "['37', 'works', 'd', 'shoulder', '#', '#', 'f', 'as', 's', 'at', 'p', '#', '#', 'pet', '2', '1', 'upper', 'with', 'car', '39', 'back', 'shoulder', 'tight', 'his', 'fire', '#', '#', 'u', '##ro', 'r', 'day', 'yo', 'worse', 'pain', 'yo', '#', '#', 'f', 'hands', 'noticed', 'hard', 'o', 'living', 'strain', '#', '#', 'm', '31', '41', 'leg', 'motion', 'hands', 'pain', 'does', '28', 'm', 'but', 'construction', 'work', 'bent', '43', 'while', 'at', 'back', 'factory', '#', '#', 't', '##re', '#', '#', 'm', 'at', '49', 'cook', 'works']\n"
     ]
    }
   ],
   "source": [
    "# Create injury report corpus, i.e. list of lists of totokenized\n",
    "\n",
    "train_report_l = train_df[\"text\"].tolist()\n",
    "training_corpus = [tokenizer.tokenize(report.lower()) for report in train_report_l]\n",
    "print(len(training_corpus))\n",
    "print(training_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training vocab dictionary\n",
    "tok_ind = 0  # numbering for dictionary\n",
    "tok_id_d = dict()  # map token to token_ind\n",
    "id_tok_d = dict()  # map token_ind to token\n",
    "\n",
    "# looping over the training corpus\n",
    "for si in range(len(training_corpus)):\n",
    "    # loop over tokens in the sentence\n",
    "    for tok in training_corpus[si]:\n",
    "        # update vocab dict\n",
    "        if tok not in tok_id_d:\n",
    "            tok_id_d[tok] = tok_ind\n",
    "            id_tok_d[tok_ind] = tok\n",
    "            # update token index\n",
    "            tok_ind += 1\n",
    "tok_id_d[\"UNK\"] = tok_ind\n",
    "id_tok_d[tok_ind] = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the vocabulary:\n",
      "7475\n",
      "7474\n",
      "chains\n"
     ]
    }
   ],
   "source": [
    "# get vocab length\n",
    "all_toks = id_tok_d.keys()\n",
    "print(\"length of the vocabulary:\")\n",
    "print(len(all_toks))\n",
    "print(tok_id_d[\"UNK\"])\n",
    "print(id_tok_d[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparring the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapTokInd(tok, tokMap):\n",
    "    if tok in tokMap:\n",
    "        return tokMap[tok]\n",
    "    else:\n",
    "        return tokMap[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train, y_train\n",
    "train_corpus_frag = training_corpus\n",
    "x_train = np.empty((len(train_corpus_frag), len(all_toks)))\n",
    "y_train = train_df[\"event\"].to_numpy()\n",
    "\n",
    "# looping over the training corpus\n",
    "for si in range(len(train_corpus_frag)):\n",
    "    word_count_sent = np.zeros(len(all_toks))\n",
    "    # loop over tokens in the sentence\n",
    "    for tok in training_corpus[si]:\n",
    "        tokInd = MapTokInd(tok, tok_id_d)\n",
    "        word_count_sent[tokInd] += 1\n",
    "    x_train[si] = word_count_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent training the corpus: 12.14181399345398 seconds\n"
     ]
    }
   ],
   "source": [
    "NBModel = MultinomialNB()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "NBModel.fit(x_train, y_train)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the time spent\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time spent training the corpus: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Accuracy and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22982\n",
      "['patient', '#', '#', 's', '51', 'and', '32', 'o', '#', '#', 'x', '#', '#', 't', '#', '#', 'f', 'wrist', '#', '#', 'f', '4', '#', '#', 'yo', 'and', 'lifting', 'bakery', '29', 'back', 'ne', '#', '#', 'y', '##m', 'upper', '#', '#', 'um', '##ba', '#', '#', 'r', '##s', 'and', 'at', 's', 'd', '#', '#', 'c', '#', '#', 'm', 'l', '#', '#', 'x', 'who', 'to', 'at', 'strain', 'r', '35', '50', 'of', 'strained', 'work', 'and', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Create test corpus\n",
    "\n",
    "test_report_l = test_df[\"text\"].tolist()\n",
    "test_corpus = [tokenizer.tokenize(report.lower()) for report in test_report_l]\n",
    "print(len(test_corpus))\n",
    "print(test_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.empty((len(test_corpus), len(all_toks)))\n",
    "y_test = test_df[\"event\"].to_numpy()\n",
    "\n",
    "# looping over the training corpus\n",
    "for si in range(len(test_corpus)):\n",
    "    word_count_sent = np.zeros(len(all_toks))\n",
    "    # loop over tokens in the sentence\n",
    "    for tok in test_corpus[si]:\n",
    "        tokInd = MapTokInd(tok, tok_id_d)\n",
    "        word_count_sent[tokInd] += 1\n",
    "    x_test[si] = word_count_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PREDICTION is done using the TEST data\n",
    "\n",
    "y_pred = NBModel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71, 63, 63, ..., 55, 27, 99])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.7666869724132"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == y_pred) / x_test.shape[0] * 100 \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Accuracy and Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9163856496388478\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for the training data\n",
    "y_train_pred = NBModel.predict(x_train)\n",
    "\n",
    "# Calculate accuracy on the training data\n",
    "train_accuracy = np.sum(y_train == y_train_pred) / x_train.shape[0]\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy for the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22982\n",
      "['radiating', '26', '#', '#', 'm', 'back', 'at', 'and', 'work', 'd', 'l', '33', '#', '#', 'p', '##ra', '#', '#', 'in', 'sitting', '#', '#', 'x', 'd', 'd', 'went', 'foot', 'ankle', '#', '#', 'x', 'w', 'p', 'facility', 'work', 'works', 'o', 'on', 'climbs', 'd', 'just', 'and', 'work', 'injury', '30', 'shelf', 'muscle', 'time', 'has', 'she', 'o', 'when']\n"
     ]
    }
   ],
   "source": [
    "dev_report_l = dev_df[\"text\"].tolist()\n",
    "dev_corpus = [tokenizer.tokenize(report.lower()) for report in dev_report_l]\n",
    "print(len(dev_corpus))\n",
    "print(dev_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dev = np.empty((len(dev_corpus), len(all_toks)))\n",
    "y_test_dev = dev_df[\"event\"].to_numpy()\n",
    "\n",
    "# looping over the training corpus\n",
    "for si in range(len(dev_corpus)):\n",
    "    word_count_sent_dev = np.zeros(len(all_toks))\n",
    "    # loop over tokens in the sentence\n",
    "    for tok in dev_corpus[si]:\n",
    "        tokInd = MapTokInd(tok, tok_id_d)\n",
    "        word_count_sent_dev[tokInd] += 1\n",
    "    x_test_dev[si] = word_count_sent_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PREDICTION is done using the TEST data\n",
    "\n",
    "y_pred_dev = NBModel.predict(x_test_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73, 73, 42, ..., 71, 62, 26])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9086676529457837"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_test_dev == y_pred_dev) / x_test_dev.shape[0]\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
